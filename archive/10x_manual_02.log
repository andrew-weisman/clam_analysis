
Load Dataset
label column: label
label dictionary: {'pole': 0, 'msi': 1, 'lcn': 2, 'p53': 3}
number of classes: 4
slide-level counts:  
 0    20
1    17
2    15
3    17
Name: label, dtype: int64
Patient-LVL; Number of samples registered in class 0: 20
Slide-LVL; Number of samples registered in class 0: 20
Patient-LVL; Number of samples registered in class 1: 17
Slide-LVL; Number of samples registered in class 1: 17
Patient-LVL; Number of samples registered in class 2: 15
Slide-LVL; Number of samples registered in class 2: 15
Patient-LVL; Number of samples registered in class 3: 17
Slide-LVL; Number of samples registered in class 3: 17
split_dir:  splits/idibell_100
################# Settings ###################
num_splits:  10
k_start:  -1
k_end:  -1
task:  idibell
max_epochs:  200
results_dir:  /home/weismanal/projects/idibell/repo/results/10x/bwh_resection/training
lr:  0.0002
experiment:  2022-04-27-2-10x
reg:  1e-05
label_frac:  1.0
bag_loss:  ce
seed:  1
model_type:  clam_sb
model_size:  small
use_drop_out:  True
weighted_sample:  True
opt:  adam
bag_weight:  0.7
inst_loss:  svm
B:  8
split_dir:  splits/idibell_100

Training Fold 0!

Init train/val/test splits... 
Done!
Training on 47 samples
Validating on 11 samples
Testing on 11 samples

Init loss function... Done!

Init Model... Setting tau to 1.0
Done!
CLAM_SB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=1, bias=True)
    )
  )
  (classifiers): Linear(in_features=512, out_features=4, bias=True)
  (instance_classifiers): ModuleList(
    (0): Linear(in_features=512, out_features=2, bias=True)
    (1): Linear(in_features=512, out_features=2, bias=True)
    (2): Linear(in_features=512, out_features=2, bias=True)
    (3): Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): SmoothTop1SVM()
)
Total number of parameters: 793869
Total number of trainable parameters: 793869

Init optimizer ... Done!

Init Loaders... Done!

Setup EarlyStopping... Done!


batch 19, loss: 0.9092, instance_loss: 1.0231, weighted_loss: 0.9433, label: 2, bag_size: 113492
batch 39, loss: 1.8537, instance_loss: 0.9303, weighted_loss: 1.5767, label: 1, bag_size: 23122


class 0 clustering acc 0.9534574468085106: correct 1434/1504
class 1 clustering acc 0.03723404255319149: correct 14/376
Epoch: 0, train_loss: 1.4040, train_clustering_loss:  1.0297, train_error: 0.6596
class 0: acc 0.0, correct 0/9
class 1: acc 0.0, correct 0/9
class 2: acc 1.0, correct 16/16
class 3: acc 0.0, correct 0/13

Val Set, val_loss: 1.4476, val_error: 0.8182, auc: 0.6910
class 0 clustering acc 1.0: correct 352/352
class 1 clustering acc 0.0: correct 0/88
class 0: acc 0.0, correct 0/3
class 1: acc 0.0, correct 0/3
class 2: acc 1.0, correct 2/2
class 3: acc 0.0, correct 0/3
Validation loss decreased (inf --> 1.447559).  Saving model ...


batch 19, loss: 1.3315, instance_loss: 0.6949, weighted_loss: 1.1405, label: 2, bag_size: 44015
batch 39, loss: 1.2080, instance_loss: 0.7239, weighted_loss: 1.0628, label: 1, bag_size: 47433


class 0 clustering acc 1.0: correct 1504/1504
class 1 clustering acc 0.0: correct 0/376
Epoch: 1, train_loss: 1.3991, train_clustering_loss:  0.7489, train_error: 0.6809
class 0: acc 0.0, correct 0/11
class 1: acc 0.4666666666666667, correct 7/15
class 2: acc 0.1, correct 1/10
class 3: acc 0.6363636363636364, correct 7/11

Val Set, val_loss: 1.3705, val_error: 0.7273, auc: 0.7535
class 0 clustering acc 1.0: correct 352/352
class 1 clustering acc 0.0: correct 0/88
class 0: acc 0.0, correct 0/3
class 1: acc 1.0, correct 3/3
class 2: acc 0.0, correct 0/2
class 3: acc 0.0, correct 0/3
Validation loss decreased (1.447559 --> 1.370536).  Saving model ...


batch 19, loss: 1.5428, instance_loss: 0.6625, weighted_loss: 1.2787, label: 2, bag_size: 75088
batch 39, loss: 1.4896, instance_loss: 0.6597, weighted_loss: 1.2406, label: 2, bag_size: 54449


class 0 clustering acc 1.0: correct 1504/1504
class 1 clustering acc 0.0: correct 0/376
Epoch: 2, train_loss: 1.3899, train_clustering_loss:  0.6419, train_error: 0.7021
class 0: acc 0.0, correct 0/11
class 1: acc 0.8571428571428571, correct 12/14
class 2: acc 0.0, correct 0/10
class 3: acc 0.16666666666666666, correct 2/12

Val Set, val_loss: 1.3683, val_error: 0.5455, auc: 0.6771
class 0 clustering acc 1.0: correct 352/352
class 1 clustering acc 0.0: correct 0/88
class 0: acc 0.0, correct 0/3
class 1: acc 0.6666666666666666, correct 2/3
class 2: acc 0.0, correct 0/2
class 3: acc 1.0, correct 3/3
Validation loss decreased (1.370536 --> 1.368294).  Saving model ...


python /home/weismanal/projects/idibell/links/clam_installation/repo/main.py --drop_out --early_stopping --lr 2e-4 --k 10 --label_frac 1 --exp_code 2022-04-27-2-10x --weighted_sample --bag_loss ce --inst_loss svm --task idibell --model_type clam_sb --log_data --subtyping --data_root_dir /home/weismanal/projects/idibell/repo/results/10x/bwh_resection --results_dir /home/weismanal/projects/idibell/repo/results/10x/bwh_resection/training
