
Load Dataset
label column: label
label dictionary: {'pole': 0, 'msi': 1, 'lcn': 2, 'p53': 3}
number of classes: 4
slide-level counts:  
 0    3
1    1
2    5
3    7
Name: label, dtype: int64
Patient-LVL; Number of samples registered in class 0: 3
Slide-LVL; Number of samples registered in class 0: 3
Patient-LVL; Number of samples registered in class 1: 1
Slide-LVL; Number of samples registered in class 1: 1
Patient-LVL; Number of samples registered in class 2: 5
Slide-LVL; Number of samples registered in class 2: 5
Patient-LVL; Number of samples registered in class 3: 7
Slide-LVL; Number of samples registered in class 3: 7
split_dir:  splits/idibell_75
################# Settings ###################
num_splits:  10
k_start:  -1
k_end:  -1
task:  idibell
max_epochs:  200
results_dir:  /home/weismanal/notebook/2021-11-11/testing_clam/results/pinyi/training
lr:  0.0002
experiment:  idibell_CLAM_75
reg:  1e-05
label_frac:  0.75
bag_loss:  ce
seed:  1
model_type:  clam_sb
model_size:  small
use_drop_out:  True
weighted_sample:  True
opt:  adam
bag_weight:  0.7
inst_loss:  svm
B:  8
split_dir:  splits/idibell_75

Training Fold 0!

Init train/val/test splits... 
Done!
Training on 12 samples
Validating on 0 samples
Testing on 0 samples

Init loss function... Done!

Init Model... Setting tau to 1.0
Done!
CLAM_SB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=1, bias=True)
    )
  )
  (classifiers): Linear(in_features=512, out_features=4, bias=True)
  (instance_classifiers): ModuleList(
    (0): Linear(in_features=512, out_features=2, bias=True)
    (1): Linear(in_features=512, out_features=2, bias=True)
    (2): Linear(in_features=512, out_features=2, bias=True)
    (3): Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): SmoothTop1SVM()
)
Total number of parameters: 793869
Total number of trainable parameters: 793869

Init optimizer ... Done!

Init Loaders... Done!

Setup EarlyStopping... Done!




class 0 clustering acc 0.7734375: correct 297/384
class 1 clustering acc 0.07291666666666667: correct 7/96
Epoch: 0, train_loss: 1.3611, train_clustering_loss:  1.2549, train_error: 0.5000
class 0: acc 0.0, correct 0/2
class 1: acc 1.0, correct 6/6
class 2: acc 0.0, correct 0/3
class 3: acc 0.0, correct 0/1
Traceback (most recent call last):
  File "/home/weismanal/notebook/2021-11-10/testing_clam/repo/main.py", line 227, in <module>
    results = main(args)
  File "/home/weismanal/notebook/2021-11-10/testing_clam/repo/main.py", line 49, in main
    results, test_auc, val_auc, test_acc, val_acc  = train(datasets, i, args)
  File "/gpfs/gsfs10/users/weismanal/notebook/2021-11-10/testing_clam/repo/utils/core_utils.py", line 186, in train
    early_stopping, writer, loss_fn, args.results_dir)
  File "/gpfs/gsfs10/users/weismanal/notebook/2021-11-10/testing_clam/repo/utils/core_utils.py", line 437, in validate_clam
    val_error /= len(loader)
ZeroDivisionError: float division by zero
